17-07-2025 00:56:34:
    1) Begins the project. Current plan
        1.1) Phase 1 : Create the main brain, the main lifeline of the model, the recommendation model to find career choice. 
        1.2) Phase 2 : Creating a SHAP explanation and linking it to a generative AI model that can create explanations and give it in the form of human readable text to make it understandable. 
        1.3) Phase 3 : Creating a backend that is capable of accepting the user input and running the model to generate and send the results. 
        1.4) Phase 4 : Create a smooth frontend that can access this backend to take the input and show the results. 

17-07-2025 01:39:22:

    2) Phase 1 initialised :
        2.1) First step : Data acquisition. Source : O-NET resources for occupation data and skills. 
        2.2) Second step : Filtering the dataset to find relevant skills and job descriptions to find good jobs. 
        2.3) Third step : Creating randomised data. Number of roles as many as possible - 150 user profiles per job. 845 total jobs and 126750 user profiles made
        2.4) Training a XGBoost model to classify and predict top 3 career choices. 
    
        2.5) First attempt : 
            2.5.1) Have 845 jobs and 126750 profiles. Drastic failure with just a 10.67% accuracy. Harsh rejection.
            2.5.2) Reason : 845 classifications is a very big multi class challenge for XGBoost. 
            2.5.3) Update for next step : First, cluster similar jobs into bigger classifications and try to find the most suitable matching cluster. If the cluster is found, search for a job title within that cluster. Thus, we create a 2 stage classifier. 
        
17-07-2025 04:19:53:
        2.6) Second attempt :
            2.6.1) First painful step was to make cluster of all the jobs. Took almost 3 hours but it is done now. Was idiotically doing it manually but then, I realised a semantic analysis can be done for all the jobs. 
            2.6.2) Failure yet again. Accuracy barely crossed 17% for stage 1 cluster identification. After that, tried to remove the interests columns, reducing the number of clusters, reducing the number of profiles but still no success.
            2.6.3) Probable reason : Skills are too overlapping to make a proper segregation. 
            2.6.4) Update for next step : Select top 500 career paths. Selection criterion is that most number of skills are required and if two jobs have same number of skills, the importance level should be considered. 

17-07-2025 22:09:41:
        2.7) Third attempt :
            2.7.1) This time, the number of jobs would be limited to 250 and 7 clusters. 
            2.7.2) Drastically better results. Sudden sweet jump from 13-14% to almost 36%. 
            2.7.3) Next step for better results : Use SHAP to identify which features help the most in model training to reduce the noise in the dataset.

18-07-2025 02:54:13:
        2.8) Fourth attempt : 
            2.8.1) Using the SHAP to find out the most contributing features and removing the unnecessary ones to reduce the noise. 
            2.8.2) Result : Still same. No visible difference in increased accuracy. So, two solutions possible are using the current 35% accuracy as a base model and predicting the job which increases the accuracy to upto 60-70% but this ain't enough. So, let this be a fall back option and going to try a deep neural network for now. If the Deep neural network fails, fall back to current 35% as the base model. 
        
18-07-2025 03:44:43:
        2.9) Fifth attempt : 
            2.9.1) Going to use a DNN model to make predictions hoping for better accuracy.
            2.9.2) Result : No better accuracy. Still hitting the limit of 36.28% which ain't better. So, the final step now is to use the best xgboost model to identify top 3 clusters and creating DNN for each cluster to find 2 most suitable jobs within those clusters. So, in total we get one xgb model for predicting clusters and 7 dnn models to identify jobs within that cluster.

18-07-2025 03:58:31:
        2.10) Sixth attempt : 
            2.10.1) Going to use the latest xgb model that gave 35% accuracy as the base model to select top three clusters. Then, we will train 7 dnn models that specialise in each cluster to predict 2 jobs per cluster. 
            2.10.2) Getting a good accuracy ranging from 58% - 65% for 4 clusters and 10 jobs each. 
        2.11) Seventh attempt : 
            2.11.1) Make clusters based on the necessary clusters and ran the brain once again. Didn't have much hope of improvement. 
            2.11.2) Got a whoppingly high accuracy of 69% - 82%
            
            

All files : 
    Dataset/Occupation_Data.txt and Skills.txt : Raw data from ONet : https://www.onetcenter.org/database.html
    Dataset/unique_job_list.csv : List of all unique 845 jobs in our database 
    Phase1_Model_Creation/career_profiles.csv : Randomly created job profiles with interests, skills and job labels
    Phase1_Model_Creation/clustered_job_titles.csv : Classifying the jobs into suitable clusters
    Phase1_Model_Creation/final_dataset.csv : Merging 3 and 4
    Phase1_Model_Creation/P1_brainA1.py : First attempt in making the model. Reference 2.5
    Phase1_Model_Creation/P1_brainA2S1.py : Second attempt in making the model. Reference 2.6
    Phase1_Model_Creation/P1_brainA3S1.py : Third attempt in making the model. Reference 2.7
    Phase1_Model_Creation/P1_brainA4S1.py : Fourth attempt. Reference 2.8
    Phase1_Model_Creation/P1_dnn_A1.py : Fifth attemp. Reference 2.9
    Phase1_Model_Creation/P1_Job_Title_Clustering.py : Semantic analysis to classify the jobs into clusters 
    Phase1_Model_Creation/P1_merger.py : Merging 3 and 4
    Phase1_Model_Creation/P1_SHAP_model.py : Recurating the dataset to remove unnecessary noise. Reference 2.8.1. Prerequisite for step 2.8.
    Phase1_Model_Creation/shap_feature_importance.csv : List of features and their contribution. 
    Preprocess/Get_Unique.py : Get list of all unique jobs
    Preprocess/Profile_Generator.py : Generate random profiles (150 per job title)